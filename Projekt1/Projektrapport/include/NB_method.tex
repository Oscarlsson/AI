
Naive Bayes was implemented using the prior probabilities $P(x_i\vert c)$ used in the Naive Bayes model $P(c\vert\mathbf{x})$, defined previously in Equation \ref{eq:naivebayes_model}. These were interpreted as the individual probabilities of each word $w$ from a vocabulary $\mathbf{V}$ occurring in class $c$, by exchanging $\left\{x_i : 1 \le i \le D\right\}$ for $\left\{w_i \in \mathbf{V}\right\}$ when using the model.
\\\\
The implementation of $P(w_i\vert c)$ follows the maximum likelihood estimate of relative frequency as follows:
\begin{align}
P(w_i\vert c) =
\frac
	{
		\sum_{d \in \mathbf{D}} F_{w_i}^{(d)}
	}
	{
		\sum_{w_j' \in \mathbf{V}} \sum_{d \in \mathbf{D}}  F_{w_j'}^{(d)}
	},
\end{align}
where $F_w^{(d)}$ defines the relative word frequency in document $d$ from corpus $\mathbf{D}$, implemented with the following two options to be compared:
\begin{description}
  \item[Binary frequency:] Boolean $\in \left\{0,1\right\}$ if word $w$ occurs in document $d$,
  \item[Tf-idf:] $\frac{\texttt{Normalized term frequency}}{\texttt{Document frequency}} \in \left[0, \log \left\vert \mathbf{D}\right\vert\right]$.
\end{description}
A caveat with this procedure is that since both of these options allow for $F_{w_i}^{(d)}$ to be zero, the product in the Naive Bayes model (\ref{eq:naivebayes_model}) for $P(c\vert d)$ vanishes for all combinations of classes $c$ and documents $d$ for which $d$ includes a word $w_i$ that never occurs in the given class. To assure nonzero prior probabilities Laplace smoothing was used, which adds one to each count as follows \citep{nb_ref}. 

\begin{align}
P(w_i\vert c) =
\frac
	{
		1 + \sum_{d \in \mathbf{D}} F_{w_i}^{(d)}
	}
	{
		\left\vert\mathbf{V}\right\vert + \sum_{w_j' \in \mathbf{V}} \sum_{d \in \mathbf{D}} F_{w_j'}^{(d)}
	},
\end{align}
The parameter $P(c)$ is implemented as
\begin{align}
P(c) = \frac{\left\vert\mathbf{D}_c\right\vert}{\left\vert\mathbf{D}\right\vert}
\end{align}
where $\left\vert\mathbf{D}_c\right\vert$ is the number of documents with class $c$. The training algorithm is shown in Algorithm \ref{algorithm:naive_bayes_training}.

\begin{algorithm}[h]
 \SetAlgoLined
 
 \KwData{\textit{Word Tf-idf-values} $\left\{F_{w_i}^{(d)} : d \in \mathbf{D}, 1 \le i \le \left\vert\mathbf{V}\right\vert\right\}$;
 		 \textit{Training classes} $\left\{c_d : d \in \mathbf{D}, c \in \mathbf{C}\right\}$; \textit{Using Tf-idf boolean} \texttt{use\_tfidf}}
 \KwResult{$P(c), P(w\vert c)$}
  \For{$i=1$ \KwTo $\left\vert\mathbf{C}\right\vert$}
  {
  	class $\leftarrow c_i$\\
  	P(class) = $\frac{\left\vert\mathbf{D}_c\right\vert}{\left\vert\mathbf{D}\right\vert}$
  }
 \ForEach{$d \in \mathbf{D}$}
 {
	class $\leftarrow c_d$\\
	\For{$i=1$ \KwTo $\left\vert\mathbf{V}\right\vert$}
	{
		word $\leftarrow w_i$ \\
		\uIf{\texttt{use\_tfidf}}
		{
			$P($word$\vert$class$)$ += $T_{w_i}^{(d)}$
		}
		\Else
		{
			$P($word$\vert$class$)$ += $1$
		}
	}
 }
 Normalize($\left\{P(w\vert c) : w \in \mathbf{V}, c \in \mathbf{C}\right\}$)\\
 \Return $\left\{P(c), c \in \mathbf{C}\right\}, \left\{P(w\vert c) : w \in \mathbf{V}, c \in \mathbf{C}\right\}$
 \caption{Multinomial Naive Bayes training algorithm}
 \label{algorithm:naive_bayes_training}
\end{algorithm}
