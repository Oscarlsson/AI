\chapter{Architecture}
In our project we implemented four different classification algorithms: Naive Bayes, Perceptron, Average Perceptron, K-nearest neighbour and used one pre-implemented Support Vector Machines. 
We compared these algorithms in classification-performance and in timing, on different feature vectors.
The different feature vectors that we studied was two different n-grams: Unigram and bigram with TF-IDF and binary values as numerical values. We also compared the result with and without the snowball-stemmer.
%Describe the different parts of your program suite in detail.
%kort inledning till kapitlet. vad har vi gjort i projektet?\\
%trattigt namn, borde kanske ändras. ska innehålla typ metod - alltså vad har vi gjort?
%\section{Finished work}
%Running modules
%What does your running code do? what is the output?
\section{Feature Vector / Text Representation}
\input{include/text_categorization}
%Stop-words \\
%TF-IDF \\
%Snowball, $https://pypi.python.org/pypi/PyStemmer/1.1.0$ \\
%Ta bort tecken
%ngrams
\section{Naive Bayes}
\input{include/NB_method}

\section{K-Nearest Neighbour (KNN) algorithm}
\input{include/KNN_metod}
\section{Support Vector Machine (SVM)}
We choose to use the matlab library $svmtrain$ \citep{svmtrain_ref} from the Statistics toolbox. Svmtrain was used hard-margined and configured such that when it didn't converge we set the classification to all zero, which would represent $100\%$ misclassification. \\
\begin{algorithm}[H]
\label{algorithm:SVM}
\SetAlgoLined
\KwData{Xtraining: The data to train, Ytraining: labels of data to train \\ Xtest: The data to test}
\KwResult{Vector containing the classified labels}
\SetKwData{Try}{try}
\SetKwData{Catch}{catch}
\SetKwData{End}{end}
\SetKwIF{Try}{Elseif}{Catch}{try}{:}{else if}{catch:}{}

Set max iterations 150000 \\
\Try{}{
svm$\_$model = $svmtrain(Xtraining, Ytraining, linear)$ \\
classifications = $svmclassify(svm\_model, Xtest)'$
}\Catch{$classifications = zeros( Xtest)$}{}{end}
 \caption{SVM using svmtrain}
\end{algorithm}



\section{The Perceptron algorithm}
The Perceptron algorithm works as follows:
The output, $w$, from the algorithm is a weight vector with the same size as $y$. If the classification for a $x$ in the training set is equal to $y$, the weight is unchanged.
If $y$ is 1 and the classification -1, then $w$ is increased. If $y$ is -1 and the classification 1, then $w$ is decreased. \citep{perceptron_ai} Algorithm~\ref{algorithm:perceptron} shows the pseudo code for the Averaged Perceptron. The Averaged Perceptron and the usual Perceptron works in the same way except that the Averaged Perceptron returns the average weight vector, $wa$, instead of the final, $w$.
This averaged vector is built incrementally by updating it while we build the usual weight vector. The pseudocode almost the same as for the usual perceptron.
\begin{algorithm}[h!]
 \SetAlgoLined
 \KwData{Xtraining: The data to train, Ytraining: labels of data to train}
 \KwResult{Weight vector $wa$}
 Initialize weight vector $w$ and averaged weight vector $wa$ to all zeros\;
 Initialize the counter $c$ to 0\;
 $N \leftarrow$ number of iterations through training set\;
 \For{1 $\rightarrow$ N}{
   \ForAll{x in Xtraining, y in Ytraining}{
    $guess \leftarrow sign(x \cdot w)$\;
    \If{$guess \neq y$}{
     $w \leftarrow w + (y \cdot x)$\;
     $average\_weight \leftarrow (N\cdot T - c) / (N\cdot T)$\;
     $wa \leftarrow wa + average\_weight \cdot (y \cdot x)$\;
     }
     Increase the counter $c$\;
     
   }
 }
 \Return $wa$
 \caption{Averaged Perceptron}
 \label{algorithm:perceptron}
\end{algorithm} \\
In both of the Perceptron algorithms, the algorithm makes a number of iterations. To decide how many iterations that must be done to get a fair result the misclassifications and the number of iterations were plotted, which is shown in Figure~\ref{fig:number_iterations}. The algorithms uses input data of 2000 words (unigram) and 10-fold crossvalidaion. As seen in the plot, both the algorithms converge after around 15 iterations for both sentimental classification and text categorization.
\begin{figure}[H]
\centering
\includegraphics[scale = 0.5]{fig/perceptron_2000words_unigram_10foldcv_classes-high_sentimental-low.png}
\caption{Plot over misclassifications for different \#iterations}
\label{fig:number_iterations}
\end{figure}\\
Since the Perceptron algorithms are 0/1 classifiers there is some difficulties with classifying the data as classes. This was solved by for all data and all classes classify the data as belonging to this class or not. Finally the new document is classified as the class it belongs to most.

