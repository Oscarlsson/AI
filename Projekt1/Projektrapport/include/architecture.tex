\chapter{Architecture}
In our project four different classification algorithms were implemented: Naive Bayes, Perceptron, Average Perceptron, K-Nearest Neighbour. Also one pre-implemented Support Vector Machines was used. 
These algorithms were compared in classification-performance and in timing, on different feature vectors.
The different feature vectors that were studied were two different n-grams: Unigram and bigram with TF-IDF and binary values as numerical values. We also compared the result with and without the snowball-stemmer.
%Describe the different parts of your program suite in detail.
%kort inledning till kapitlet. vad har vi gjort i projektet?\\
%trattigt namn, borde kanske ändras. ska innehålla typ metod - alltså vad har vi gjort?
%\section{Finished work}
%Running modules
%What does your running code do? what is the output?
\section{Programs and tools}
Python was used to generate the feature vector from the documents. 
\\\\
Matlab was used to implement the algorithms, Cross-validation and to generate plots. The Statistics Toolbox with functions $cvpartition$, to easily perform N-cross-validation and $svmtrain$ to train a SVM, has been of great use during the project. Statistics Toolbox also contains a handy function to perform Principal Component Analysis, which was a help tool to visualize data with many variables. 

\section{Feature Vector / Text Representation}
\input{include/text_categorization}
%Stop-words \\
%TF-IDF \\
%Snowball, $https://pypi.python.org/pypi/PyStemmer/1.1.0$ \\
%Ta bort tecken
%ngrams
\section{Naive Bayes}
\input{include/NB_method}

\section{K-Nearest Neighbour (KNN)}
\input{include/KNN_metod}

\section{Support Vector Machine (SVM)}
The pre-implemented Support Vector Machine used the matlab library $svmtrain$ \citep{svmtrain_ref}, from the Statistics toolbox, seen in Algorithm~\ref{algorithm:SVM}.  Svmtrain was configured to be soft margined, penalty parameter $C = 1$, and such that when it did not converge the classification was set to zero, which would represent $100\%$ misclassification. A linear kernel function was used. \\
\begin{algorithm}[H]
\SetAlgoLined
\KwData{Xtraining: The data to train, Ytraining: labels of data to train \\ Xtest: The data to test}
\KwResult{Vector containing the classified labels}
\SetKwData{Try}{try}
\SetKwData{Catch}{catch}
\SetKwData{End}{end}
\SetKwIF{Try}{Elseif}{Catch}{try}{:}{else if}{catch:}{}

Set max iterations 150000 \\
\Try{}{
svm$\_$model = $svmtrain(Xtraining, Ytraining, linear)$ \\
classifications = $svmclassify(svm\_model, Xtest)'$
}\Catch{$classifications = zeros( Xtest)$}{}{end}
 \caption{SVM using svmtrain}
 \label{algorithm:SVM}
\end{algorithm}

\section{The Perceptron algorithm}
The Perceptron algorithm was implemented as follows:
The output $w$ from the algorithm was a weight vector with the same size as $y$. If the classification for a $x$ in the training set is equal to $y$, the weight was unchanged.
If $y$ was 1 and the classification -1, then $w$ was increased. If $y$ was -1 and the classification 1, then $w$ was decreased \citep{perceptron_ai}. Algorithm~\ref{algorithm:perceptron} shows the pseudocode for the Averaged Perceptron. The Averaged Perceptron and the usual Perceptron works in the same way except that the Averaged Perceptron returns the average weight vector $wa$ instead of the final $w$.
This averaged vector was built incrementally by updating it while the usual weight vector was built. The pseudocode is almost the same as for the usual perceptron.
\begin{algorithm}[h!]
 \SetAlgoLined
 \KwData{Xtraining: The data to train, Ytraining: labels of data to train}
 \KwResult{Weight vector $wa$}
 Initialize weight vector $w$ and averaged weight vector $wa$ to all zeros\;
 Initialize the counter $c$ to 0\;
 $N \leftarrow$ number of iterations through training set\;
 \For{1 $\rightarrow$ N}{
   \ForAll{x in Xtraining, y in Ytraining}{
    $guess \leftarrow sign(x \cdot w)$\;
    \If{$guess \neq y$}{
     $w \leftarrow w + (y \cdot x)$\;
     $average\_weight \leftarrow (N\cdot T - c) / (N\cdot T)$\;
     $wa \leftarrow wa + average\_weight \cdot (y \cdot x)$\;
     }
     Increase the counter $c$\;
     
   }
 }
 \Return $wa$
 \caption{Averaged Perceptron}
 \label{algorithm:perceptron}
\end{algorithm} \\
In both the Perceptron algorithms, the algorithm makes a number of iterations. To decide how many iterations that must be done to get a fair result the misclassifications and the number of iterations were plotted, which is shown in Figure~\ref{fig:number_iterations}. The algorithms used input data of 2000 words (unigram) and 10-fold crossvalidaion. As seen in the plot, both the algorithms converged after around 15 iterations for both sentimental classification and the classification for different classes. 
\begin{figure}[h!]
\centering
\includegraphics[scale = 0.5]{../Plottar/perceptron_2000words_unigram_10foldcv_classes-high_sentimental-low.png}
\caption{Plot over misclassifications for different \#iterations}
\label{fig:number_iterations}
\end{figure}\\
Since the Perceptron algorithms are 0/1 classifiers there were some difficulties with classifying the data as classes. This was solved by: for all data and all classes classify if data belongs to this class or not. Finally choose the class it belonged to most.
