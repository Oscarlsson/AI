\chapter{Conclusions}
Sum up your project, suggest future extensions and improvements.
\\\\
future extensions: se future work

\section{Future work}
It would have been interesting to study the correlation with another dataset than the Amazon.  \\\\
The bigram-feature gave bad results and we didn't have time to investigate this further but one could continue where we stopped and try to achieve better classification using bigram. \\\\
Boosting is a meta-algorithm in supervised learning which takes a set of weak learners and creates a single large learner. (ref till http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf) It would have been interesting to e.g implement AdaBoost and messure the performance gain. \\\\
K-nearest neighbor gave worse classification than the other algorithms and it would have been interesting to investigate if some parameters would turn the situation around and make KNN better. We didn't have time to investigate if some feature selection would make KNN equivalent to the others.
%Modules a future continuation may have
%\\\\
%* annat dataset\\
%* fler ord\\
%* kombinera algoritmer\\
%* boosting\\
