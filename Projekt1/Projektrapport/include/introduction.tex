\chapter{Introduction}
This project is about machine learning of different statistical classifiers.
Machine learning is about construct and study systems that can learn from data.
This could be used to train a system to recognize patterns in, for instance, emails and learn how to distinguish between spam and non-spam messages. In this project the classifiers are applied to reviews and the task is to categorize these.
By statistical analysis of experiments, the classifiers are compared in terms of, for instance, classification accuracy.

\section{Problem description}
The problem is to categorize reviews from Amazon in 6 different categories; books, camera, DVD, health, music and software.
The classification is done by implementing five different classification algorithms; Naive Bayes, Perceptron, Averaged Perceptron, K-nearest neighbours (KNN) and Support vector machine (SVM). \\\\
There is three different kinds of classification tests that should be done: 
\begin{itemize}
\item In-domain sentiment analysis - For each category, train and classify documents as positive or negative
\item Out-of-domain sentiment analysis - Train on one category and test on another
\item Text categorization - categorize the documents into their categories
\end{itemize}
Before classifying the input data, which is text files, must be processed.
The input should be divided in words somehow, either one word (unigrams) or two words (bigrams). Some common words are totally useless for the classification, such as ''the'' and ''I''. These are called stop-words and should not be a part of the input to the algorithms. To get less words it's also possible to use a stemmer, such as Snowball to prevent separation of different inflectional forms, such as ''person'' and ''persons''. Another way of getting less input data is to use Term Frequency–Inverse Document Frequency TF-IDF. TF-IDF is a numerical statistic of how important a is in a document.

\section{Theory}
Describe briefly the scientific papers or book chapters you found relevant to the problem, references to sec 6. Explain which are relevant for your project and which not and why.

\subsection{Naive Bayes}
\begin{algorithm}[h]
\label{algorithm:naive_bayes}
 \SetAlgoLined
 \KwData{this text}
 \KwResult{how to write algorithm with \LaTeX2e }
 initialization\;
 \While{not at end of this document}{
  read current\;
  \eIf{understand}{
   go to next section\;
   current section becomes this one\;
   }{
   go back to the beginning of current section\;
  }
 }
 \caption{How to write algorithms}
\end{algorithm}

\subsection{K-Nearest Neighbour (KNN) algorithm}
%http://www.cse.chalmers.se/edu/course/TDA231/mlsli11.pdf
K-nearest neighbors (KNN) algorithm is one of the simplest machine learning
algorithms. The classification performed by KNN works as follows: Fix some
number $k$. For any new $x \in X$, take the most common classification value among
the $k$ training examples in $D$ closest to $x$.
\\\\
Even though the KNN algorithm
seems extraordinary simple there are some things that one must take in
consideration. First of all what is nearest? There is of course a natural
distance function in a geometrical space but a proper definition in parameter
space is not obvious. As it turn out a distance function can be defined in a lot
of ways. The given attributes are overall very different in reality.
One may may multiply each attribute variable by arbitrary factors or in other
words stretch the coordinate axes arbitrary and independent. It is very easy to
find examples when doing so will result in very different outcomes of the classifications.
Even further irrelevant attributes may change the classification even though they
shouldn't do so. This phenomena is called \emph{curse of dimensionality}. One
can try to find good stretch factors by performing cross validation.
One takes a vector of stretch factors that
minimizes the classification errors on a valid set. The back side is that
this will require a lot of test vectors.
\\\\
An other problem is the computation complexity. KNN is sad to have a lazy
learning method. In other words it does output hypotheses in explicit form.
Instead a lot of computational steps are required for the classification.
\\\\
A further issue is to find a suitable $k$. A great choose of $k$ various
depending on many circumstances. A keystone is that if the training set is large
it's important get rid of noise in the data hence increase $k$. On the other
hand a big $k$ requires more calculation. There are however also other more
complex ways for reducing noise.
%https://dl.dropbox.com/u/5139428/ml-course/Classification1.pdf
\begin{center}
\includegraphics[scale=0.5]{fig/KNN.png}
\end{center}

\subsection{Support Vector Machine (SVM)}

\subsection{The Perceptron algorithm}
Perceptron is a linear, supervised classification algorithm. The Perceptron algorithm consider a 0/1 classification problem.
The algorithm tries to separate the input data with a linear decision boundary, as shown in Picture X. The algorithm output a weight vector which should represent the line between the two separated classes for each input data point. \citep{perceptron_url}
\begin{center}
\includegraphics[scale = 0.5]{fig/perceptron_example.png}
\end{center}
There is also another variant of the Perceptron algorithm, which is called Averaged Perceptron. They work in the same way, except that the Averaged Perceptron outputs the averaged weight vector instead of the final one.

\subsection{Text categorization}


\subsection{Programs and tools}
What tools and programs are already available for the problem, or for closely related ones?
Describe these briefly. Say how you can use them, and how your work will build on them, or differ from them. Explain which are relevant for your project and which not and why.
\\\\
* vilka program är vettiga att använda? -leta upp några-fördelar/nackdelar
skriv att vi valde matlab och varför.



